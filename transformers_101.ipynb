{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Getting started\n",
    "\n",
    "Start with a simple example with the high level pipeline API."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel\n",
    "from transformers import AutoModel\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997795224189758}]"
      ]
     },
     "metadata": {},
     "execution_count": 147
    }
   ],
   "source": [
    "classifier('We are very happy to show you the ðŸ¤— Transformers library.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "label: POSITIVE, with score: 0.9998\nlabel: NEGATIVE, with score: 0.5309\n"
     ]
    }
   ],
   "source": [
    "results = classifier([\n",
    "    \"We are very happy to show you the ðŸ¤— Transformers library.\",\n",
    "    \"We hope you don't hate it.\"\n",
    "])\n",
    "for result in results:\n",
    "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")\n"
   ]
  },
  {
   "source": [
    "# Internal\n",
    "\n",
    "Take a look at what is happening under the hood"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name, from_pt=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "classifier_i18n = pipeline('sentiment-analysis', model=tf_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'label': '5 stars', 'score': 0.7725350260734558}, {'label': '5 stars', 'score': 0.23652462661266327}]\n"
     ]
    }
   ],
   "source": [
    "pipeline_results = classifier_i18n([\n",
    "    \"We are very happy to show you the ðŸ¤— Transformers library.\",\n",
    "    \"We hope you don't hate it.\"\n",
    "])\n",
    "print(pipeline_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input_ids: [[101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102], [101, 11312, 18763, 10855, 11530, 112, 162, 39487, 10197, 119, 102, 0, 0, 0]]\ntoken_type_ids: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\nattention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "tf_batch = tokenizer(\n",
    "    [\n",
    "        \"We are very happy to show you the ðŸ¤— Transformers library.\",\n",
    "        \"We hope you don't hate it.\"\n",
    "    ],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"tf\"\n",
    ")\n",
    "for key, value in tf_batch.items():\n",
    "    print(f\"{key}: {value.numpy().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(2, 5), dtype=float32, numpy=\narray([[-2.6222003 , -2.7745316 , -0.8966624 ,  2.0137324 ,  3.3063858 ],\n       [ 0.0063588 , -0.12577434, -0.05034703, -0.16553083,  0.13285917]],\n      dtype=float32)>, hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "tf_outputs = tf_model(tf_batch)\n",
    "print(tf_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(\n[[0.00205668 0.00176608 0.01154935 0.21209283 0.7725351 ]\n [0.20841938 0.18262215 0.19692962 0.17550415 0.2365247 ]], shape=(2, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)\n",
    "print(tf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TFSequenceClassifierOutput(loss=<tf.Tensor: shape=(2,), dtype=float32, numpy=array([6.338995 , 1.7003361], dtype=float32)>, logits=<tf.Tensor: shape=(2, 5), dtype=float32, numpy=\narray([[-2.6222003 , -2.7745316 , -0.8966624 ,  2.0137324 ,  3.3063858 ],\n       [ 0.0063588 , -0.12577434, -0.05034703, -0.16553083,  0.13285917]],\n      dtype=float32)>, hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "tf_outputs_with_loss = tf_model(\n",
    "    tf_batch, \n",
    "    labels=tf.constant([1, 1])\n",
    ")\n",
    "print(tf_outputs_with_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory='./tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(save_directory)\n",
    "tf_model.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "All TF 2.0 model weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "model = AutoModel.from_pretrained(save_directory, from_tf=True)"
   ]
  },
  {
   "source": [
    "# Getting deeper\n",
    "\n",
    "Check what is the TF specific code under the hood"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_778']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "distilbert_model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "distilbert_model = TFDistilBertForSequenceClassification.from_pretrained(distilbert_model_name)\n",
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained(distilbert_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'activation_13', 'vocab_layer_norm', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'dropout_798', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "custom_label_model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}